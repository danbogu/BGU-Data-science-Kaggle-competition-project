---
title: "Projet"
author: "Dan Boguslavsky and Nadav Livnah"
date: "5/25/2020"
output: html_document
---
```{r clear environment}
rm(list = ls())
```

```{r load data}
data<-read.csv("/Users/danboguslavsky/git/datascience/trainOLD.csv")
test.data<-read.csv("/Users/danboguslavsky/git/datascience/test.csv")
```


```{r all initial data hist and data balance}
  par(mfrow = c(1,2))
  hist(data$stn_type,main="Train")
  hist(test.data$stn_type,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$stn_elev,main="Train")
  hist(test.data$stn_elev,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$climate_type,main="Train")
  hist(test.data$climate_type,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$aqua_night_lst,main="Train")
  hist(test.data$aqua_night_lst,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$aqua_emis,main="Train")
  hist(test.data$aqua_emis,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$aqua_ndvi,main="Train") #lower tail to big in train set
  hist(test.data$aqua_ndvi,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$elev,main="Train")
  hist(test.data$elev,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$pop,main="Train") # I think we should cut the train set where population is greather then 19001
  hist(test.data$pop,main="Test") 
  
  par(mfrow = c(1,2))
  hist(data$clc_artificial,main="Train")
  hist(test.data$clc_artificial,main="Test")
  
  par(mfrow = c(1,2))
  hist(data$clc_water,main="Train")
  hist(test.data$clc_water,main="Test")
  
   par(mfrow = c(1,2))
  hist(data$clc_bare,main="Train")
  hist(test.data$clc_bare,main="Test")
  
   par(mfrow = c(1,2))
  hist(data$clc_vegetation,main="Train")
  hist(test.data$clc_vegetation,main="Test")
  
  ##all rest seems balanced.
```

```{r splitting into areas}
range(data$lat)
range(data$lon)
range(data$lat)
range(data$lon)
data$region<-NA
for(lat in seq(42,52,by=1)){
  for(lon in seq(-6,9,by=1)){
    for(i in 1:nrow(data)){
      if((round(data$lat[i])==lat) & (round(data$lon[i])==lon)){
    
       data$region[i]<-paste("[",lat,",",lon,"]",sep="")
      }#if
    }#for data
}#for lon
}#for lat
data$region<-as.factor(data$region)
```

```{r splitting into areas - for test set}
range(test.data$lat)
range(test.data$lon)
#range(data$lat)
#range(data$lon)
test.data$region<-NA
for(lat in seq(42,51,by=1)){
  for(lon in seq(-5,9,by=1)){
    for(i in 1:nrow(test.data)){
      if(round(test.data$lat[i])==lat & round(test.data$lon[i])==lon){
    
       test.data$region[i]<-paste("[",lat,",",lon,"]",sep="")
      }#if
    }#for data
}#for lon
}#for lat
test.data$region<-as.factor(test.data$region)
```


Building a simple out of the blue decision tree:


Divide the data to 20% validation set:
```{r}
in_train <- sample(1:nrow(data), 0.8*nrow(data)) 
data_train <- data[in_train, ] 
data_val <- data[-in_train, ] 
```

(remove: sample id, station id, date)
```{r}
subset.1<-data[,-c(16)]
data_val<-data_val[,4:18]
```

Checking IG:
```{r}
library('FSelector')
information.gain(TempLabel~.,data=data)
```

Building Tree model /// with hyperparameter tuning
```{r}
library('rpart')
library('rpart.plot')
full.train.data<-data[,4:18]
#for(i in seq(1,10,by=3)){
tree<-rpart(TempLabel~.,data=data_train,maxdepth=8,cp=0,minbucket=20,minsplit=20)
library('FSelector')
#rpart.plot(tree)
prediction.test<- predict(tree,test.data,type = 'class')
  #con_mat<-table(data_val$TempLabel,prediction.test)
  #con_mat
  #print(acc<-(sum(diag(con_mat)) / sum(con_mat)))
  #}
```

Predictions:
```{r}
prediction.test<- predict(tree,data_val,type = 'class')
  con_mat<-table(data_val$TempLabel,prediction.test)
  con_mat
  print(acc<-(sum(diag(con_mat)) / sum(con_mat)))
```

Logistic regression:
```{r}
subset.2<-data[,4:18]
test_subset<-test.data[,4:17]
test_subset$TempLabel<-NA
#for(alpah in c(0.6,0.7,0.8,0.9))
  #total_acc.1-c()
for (i in c("A","B","C","D","E")){
  #subset.2<-data
  subset.2$TempLabel<-ifelse(subset.2$TempLabel=="E",1,0)
  #subset.test<-data_test
  #subset.test$TempLabel<-ifelse(data_test$TempLabel==i,1,0)
  glm.1<-glm(TempLabel~.,data=subset.2,family = binomial)
  data_test_hat<-predict(glm.1, test_subset, type = "response")
  data_test_hat_binar<-(data_test_hat>0.5)*1
  test_subset$TempLabel<-ifelse((data_test_hat_binar==1)&(is.na(test_subset$TempLabel)),i,test_subset$TempLabel)
  #CM.1 <- table(true= subset.test$TempLabel, predicted = data_test_hat_binar)
  #acc<-(sum(diag(CM.1)) / sum(CM.1))
  #correct<-c()
  #for(i in 1:nrow(data_test)){
  #  correct<-c(correct,subset.test$TempLabel[i]==data_test_hat_binar[i])
  #}
  #total_acc.1<-c(total_acc,acc)
}
```

```{r}
for (i in 1:nrow(predictions_class)){
  predictions_class$lable[i]<-lables[which(max(predictions_class[i,1:5],na.rm = T)==predictions_class[i,])]
}
```

LMM(Random effect):
```{r LMM V1}
library('lme4')
lmer.model<-lmer(temp~1+(aqua_night_lst+region|date),data=data_train)

prediction.test<-predict(lmer.model,newdata = data_val,allow.new.levels=T)
```

```{rLMM V1}
lmm1 <- lmer(temp ~  
                    aqua_night_lst+
                    (1|date), data = data_train)
```


```{r LMM predictions}
#prediction.test<-predict(lm.1,newdata = data_val)
prediction.test<-ifelse(prediction.test>=5,5,prediction.test)
prediction.test<-ifelse(prediction.test<=1,1,prediction.test)

prediction.test<-round.by(prediction.test)
prediction.test<-ifelse(prediction.test==1,"A",ifelse(prediction.test==2,"B",ifelse(prediction.test==3,"C",ifelse(prediction.test==4,"D","E"))))
```

```{r LMM categorisation}
data$temp<-ifelse(data$TempLabel=="A",1,ifelse(data$TempLabel=="B",2,ifelse(data$TempLabel=="C",3,ifelse(data$TempLabel=="D",4,5))))
```


```{r Acuuracy testing}
sum=0
for (i in 1:NROW(prediction.test)){
  if(prediction.test[i]==data_val$TempLabel[i]){
  sum=sum+1  
  }
}
sum/NROW(prediction.test)
```

Naive Bayes:
```{r}
library('e1071')
bayes<-naiveBayes(TempLabel~aqua_night_lst+climate_type*region,data=data)
prediction.train<-predict(bayes,newdata = data)
```

Random forest:
```{r model fit}
rf<-randomForest::randomForest(TempLabel~date+aqua_night_lst+elev+region,data=data_train,ntree=200)
```

```{r RF prediction}
prediction.train<-predict(rf,newdata = data_val)
```

MLR:
```{r MLR fit}
library('mlr')
makeMultilabelTask(id = "multi", data = data_train,target =data_train$aqua_night_lst)
```
```{r scale data}
data_scale$aqua_night_lst<-scale(data_scale$aqua_night_lst)
data_scale$temp<-scale(data_scale$temp)
```

```{r MLR predictions with tuning}
prediction.test<-ifelse(data_scale$aqua_night_lst<(-0.842),"A",ifelse(data_scale$aqua_night_lst<(-0.253)&data_scale$aqua_night_lst>(-0.842),"B",ifelse(data_scale$aqua_night_lst<(0.253)&data_scale$aqua_night_lst>(-0.253),"C",ifelse(data_scale$aqua_night_lst<(0.842)&data_scale$aqua_night_lst>(0.253),"D","E"))))
```

```{r lm}
lm.1<-lm(data_train_2$temp ~ .,data = data_train_2)
prediction.train<-predict(lm.1,data_test_2)
```

New improved LMM version:
```{r}
data$X <- NULL
test.data$X <- NULL
```


Spiting the data
```{r}
in_train <- sample(1:nrow(data), 0.8*nrow(data)) 
data_train <- data[in_train, ] 
data_validation <- data[-in_train, ] 
```


```{r}
lme1 <- lmer(Templabel_n ~ 
               stn_type + climate_type + date + g_loc + g_lat + 
               aqua_night_lst + elev_log + stn_elev_log + an_lst_sq + clc_vegetation + aqua_emis + climate_type_num + lat + clc_artificial + stn_elev + elev +
               pop_log*climate_type + (elev + an_lst_sq + aqua_night_lst + lat |date/g_loc) 
             , data = data)
y_hat <- predict(lme1, newdata = data_validation, allow.new.levels = T)
y_hat_cat <- 
  ifelse (y_hat < 1.5, 1,
        ifelse (y_hat < 2.5, 2,
                ifelse(y_hat < 3.5, 3,
                       ifelse(y_hat < 4.5, 4, 5))))

(acurate_rate_matrix <- table(true = data_validation$Templabel_n, predicted = y_hat_cat))
(accuracy <-sum(diag(acurate_rate_matrix) / sum(acurate_rate_matrix)))
```


```{r}
y_hat_cat <- 
  ifelse (y_hat < 1.655, 1,
        ifelse (y_hat < 2.448, 2,
                ifelse(y_hat < 3.468, 3,
                       ifelse(y_hat < 4.4007, 4, 5))))
(acurate_rate_matrix <- table(true = data_validation$Templabel_n, predicted = y_hat_cat))
(accuracy <-sum(diag(acurate_rate_matrix) / sum(acurate_rate_matrix)))
```

```{r New LMM tuning}

y_hat <- predict(lme1, newdata = test.data, allow.new.levels = T)
y_hat_cat <- 
ifelse (y_hat < 1.655, "A",
        ifelse (y_hat < 2.448, "B",
                ifelse(y_hat < 3.47, "C",
                       ifelse(y_hat < 4.4007, "D", "E"))))
SampleId <- test.data$SampleId
TempLabel <- c(y_hat_cat)
result <- data.frame(SampleId, TempLabel)
write.csv(result,"20_7_new.csv")      
```


```{r creating new locations}

for (i in 1:length(test.data$g_loc)){
  if (test.data$g_loc[i] == "lat_1 lon_5"){test.data$g_loc[i] <- "lat_2 lon_5"}
  else{ if(test.data$g_loc[i] == "lat_2 lon_7"){test.data$g_loc[i] <- "lat_3 lon_7"}}
}
```